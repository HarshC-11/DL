{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fec470bf",
   "metadata": {},
   "source": [
    "## Name: Vaibhav Bichave\n",
    "\n",
    "## Implement the Continuous Bag of Words (CBOW) Model for the given (textual document) using the below steps:\n",
    "    a. Data preparation\n",
    "    b. Generate training data\n",
    "    c. Train model\n",
    "    d. Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc9467d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =\"\"\"Deep learning (also known as deep structured learning) is part of a broader \n",
    "family of machine learning methods based on artificial neural networks with representation learning. \n",
    "Learning can be supervised, semi-supervised or unsupervised. Deep-learning architectures such as deep neural\n",
    "networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural \n",
    "networks and Transformers have been applied to fields including computer vision, speech recognition, natural \n",
    "language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, \n",
    "material inspection and board game programs, where they have produced results comparable to and in some cases \n",
    "surpassing human expert performance.\"\"\"\n",
    "\n",
    "data = data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb9e67e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data)\n",
    "\n",
    "word2id = tokenizer.word_index\n",
    "word2id['PAD'] = 0\n",
    "\n",
    "id2word = {v:k for k,v in word2id.items()}\n",
    "wids = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "emb_size = 100\n",
    "window_size = 2\n",
    "vocab_size = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63df0353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca027e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow_model(corpus,vocab_size, window_size):\n",
    "    context_length = window_size*2\n",
    "    for words in corpus:\n",
    "        sequences_size = len(words)\n",
    "        for index,word in enumerate(words):\n",
    "            context_word = []\n",
    "            label_word = []\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            context_word.append([words[i]\n",
    "                               for i in range(start,end)\n",
    "                               if 0<=i <sequences_size\n",
    "                               and i!=index])\n",
    "            label_word.append(word)\n",
    "            \n",
    "            x = pad_sequences(context_word,context_length)\n",
    "            y = to_categorical(label_word,vocab_size)\n",
    "            yield(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3947598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Embedding,Lambda\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bce96afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 4, 100)            7500      \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 75)                7575      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,075\n",
      "Trainable params: 15,075\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cbow = Sequential([\n",
    "    Embedding(vocab_size,emb_size,input_length = window_size*2),\n",
    "    Lambda(lambda x:K.mean(x,axis=1)),\n",
    "    Dense(vocab_size,activation = 'softmax')\n",
    "])\n",
    "\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "cbow.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4d5e66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 0 - Loss -> 435.33561515808105\n",
      "Epochs 1 - Loss -> 429.302264213562\n",
      "Epochs 2 - Loss -> 426.6885013580322\n",
      "Epochs 3 - Loss -> 423.72633481025696\n",
      "Epochs 4 - Loss -> 420.88743686676025\n",
      "Epochs 5 - Loss -> 418.6238157749176\n"
     ]
    }
   ],
   "source": [
    "for epochs in range(6):\n",
    "    loss  = 0\n",
    "    for x,y in cbow_model(corpus=wids,vocab_size = vocab_size,window_size=window_size):\n",
    "        loss += cbow.train_on_batch(x,y)\n",
    "    print(\"Epochs {} - Loss -> {}\".format(epochs,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7b2333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "weights = cbow.get_weights()[0][:]\n",
    "# pd.DataFrame(weights,index=word2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa517f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning</th>\n",
       "      <th>deep</th>\n",
       "      <th>networks</th>\n",
       "      <th>neural</th>\n",
       "      <th>and</th>\n",
       "      <th>as</th>\n",
       "      <th>of</th>\n",
       "      <th>machine</th>\n",
       "      <th>supervised</th>\n",
       "      <th>have</th>\n",
       "      <th>...</th>\n",
       "      <th>results</th>\n",
       "      <th>comparable</th>\n",
       "      <th>in</th>\n",
       "      <th>some</th>\n",
       "      <th>cases</th>\n",
       "      <th>surpassing</th>\n",
       "      <th>human</th>\n",
       "      <th>expert</th>\n",
       "      <th>performance</th>\n",
       "      <th>PAD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564340</td>\n",
       "      <td>0.520465</td>\n",
       "      <td>0.687309</td>\n",
       "      <td>0.702848</td>\n",
       "      <td>0.677681</td>\n",
       "      <td>0.736050</td>\n",
       "      <td>0.714989</td>\n",
       "      <td>0.718350</td>\n",
       "      <td>0.959826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.707415</td>\n",
       "      <td>0.642867</td>\n",
       "      <td>0.712345</td>\n",
       "      <td>0.662297</td>\n",
       "      <td>0.745983</td>\n",
       "      <td>0.728790</td>\n",
       "      <td>0.722269</td>\n",
       "      <td>0.731950</td>\n",
       "      <td>0.675630</td>\n",
       "      <td>0.765723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deep</th>\n",
       "      <td>0.564340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.743636</td>\n",
       "      <td>0.701144</td>\n",
       "      <td>0.670640</td>\n",
       "      <td>0.683945</td>\n",
       "      <td>0.715158</td>\n",
       "      <td>0.725764</td>\n",
       "      <td>0.659227</td>\n",
       "      <td>0.914696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702825</td>\n",
       "      <td>0.670269</td>\n",
       "      <td>0.678129</td>\n",
       "      <td>0.623978</td>\n",
       "      <td>0.744906</td>\n",
       "      <td>0.704478</td>\n",
       "      <td>0.693541</td>\n",
       "      <td>0.752698</td>\n",
       "      <td>0.738534</td>\n",
       "      <td>0.722181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>networks</th>\n",
       "      <td>0.520465</td>\n",
       "      <td>0.743636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.678898</td>\n",
       "      <td>0.697930</td>\n",
       "      <td>0.663260</td>\n",
       "      <td>0.728734</td>\n",
       "      <td>0.693935</td>\n",
       "      <td>0.675196</td>\n",
       "      <td>0.914873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.707060</td>\n",
       "      <td>0.634636</td>\n",
       "      <td>0.642973</td>\n",
       "      <td>0.671950</td>\n",
       "      <td>0.676299</td>\n",
       "      <td>0.738150</td>\n",
       "      <td>0.694240</td>\n",
       "      <td>0.688614</td>\n",
       "      <td>0.644455</td>\n",
       "      <td>0.709829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neural</th>\n",
       "      <td>0.687309</td>\n",
       "      <td>0.701144</td>\n",
       "      <td>0.678898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472190</td>\n",
       "      <td>0.356580</td>\n",
       "      <td>0.418329</td>\n",
       "      <td>0.407396</td>\n",
       "      <td>0.380304</td>\n",
       "      <td>0.650154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.407154</td>\n",
       "      <td>0.363231</td>\n",
       "      <td>0.405016</td>\n",
       "      <td>0.365781</td>\n",
       "      <td>0.442707</td>\n",
       "      <td>0.425865</td>\n",
       "      <td>0.427338</td>\n",
       "      <td>0.395312</td>\n",
       "      <td>0.447651</td>\n",
       "      <td>0.457611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.702848</td>\n",
       "      <td>0.670640</td>\n",
       "      <td>0.697930</td>\n",
       "      <td>0.472190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430277</td>\n",
       "      <td>0.406545</td>\n",
       "      <td>0.419735</td>\n",
       "      <td>0.382753</td>\n",
       "      <td>0.669322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.401106</td>\n",
       "      <td>0.399500</td>\n",
       "      <td>0.443710</td>\n",
       "      <td>0.380114</td>\n",
       "      <td>0.420614</td>\n",
       "      <td>0.431645</td>\n",
       "      <td>0.416170</td>\n",
       "      <td>0.421400</td>\n",
       "      <td>0.412978</td>\n",
       "      <td>0.376110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surpassing</th>\n",
       "      <td>0.728790</td>\n",
       "      <td>0.704478</td>\n",
       "      <td>0.738150</td>\n",
       "      <td>0.425865</td>\n",
       "      <td>0.431645</td>\n",
       "      <td>0.435908</td>\n",
       "      <td>0.397487</td>\n",
       "      <td>0.413895</td>\n",
       "      <td>0.392436</td>\n",
       "      <td>0.689252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395883</td>\n",
       "      <td>0.441815</td>\n",
       "      <td>0.399707</td>\n",
       "      <td>0.380512</td>\n",
       "      <td>0.441793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.440634</td>\n",
       "      <td>0.398612</td>\n",
       "      <td>0.424760</td>\n",
       "      <td>0.437916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human</th>\n",
       "      <td>0.722269</td>\n",
       "      <td>0.693541</td>\n",
       "      <td>0.694240</td>\n",
       "      <td>0.427338</td>\n",
       "      <td>0.416170</td>\n",
       "      <td>0.400384</td>\n",
       "      <td>0.435623</td>\n",
       "      <td>0.393451</td>\n",
       "      <td>0.412255</td>\n",
       "      <td>0.715151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.431959</td>\n",
       "      <td>0.377919</td>\n",
       "      <td>0.379137</td>\n",
       "      <td>0.363673</td>\n",
       "      <td>0.407000</td>\n",
       "      <td>0.440634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.422697</td>\n",
       "      <td>0.441135</td>\n",
       "      <td>0.401258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expert</th>\n",
       "      <td>0.731950</td>\n",
       "      <td>0.752698</td>\n",
       "      <td>0.688614</td>\n",
       "      <td>0.395312</td>\n",
       "      <td>0.421400</td>\n",
       "      <td>0.402563</td>\n",
       "      <td>0.424996</td>\n",
       "      <td>0.411999</td>\n",
       "      <td>0.408820</td>\n",
       "      <td>0.722799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387381</td>\n",
       "      <td>0.399963</td>\n",
       "      <td>0.357308</td>\n",
       "      <td>0.391456</td>\n",
       "      <td>0.418710</td>\n",
       "      <td>0.398612</td>\n",
       "      <td>0.422697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.413012</td>\n",
       "      <td>0.435040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>performance</th>\n",
       "      <td>0.675630</td>\n",
       "      <td>0.738534</td>\n",
       "      <td>0.644455</td>\n",
       "      <td>0.447651</td>\n",
       "      <td>0.412978</td>\n",
       "      <td>0.394644</td>\n",
       "      <td>0.406125</td>\n",
       "      <td>0.395811</td>\n",
       "      <td>0.401579</td>\n",
       "      <td>0.666266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362889</td>\n",
       "      <td>0.418886</td>\n",
       "      <td>0.409257</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.381308</td>\n",
       "      <td>0.424760</td>\n",
       "      <td>0.441135</td>\n",
       "      <td>0.413012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.401320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAD</th>\n",
       "      <td>0.765723</td>\n",
       "      <td>0.722181</td>\n",
       "      <td>0.709829</td>\n",
       "      <td>0.457611</td>\n",
       "      <td>0.376110</td>\n",
       "      <td>0.443575</td>\n",
       "      <td>0.388459</td>\n",
       "      <td>0.430730</td>\n",
       "      <td>0.370831</td>\n",
       "      <td>0.687655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408633</td>\n",
       "      <td>0.424597</td>\n",
       "      <td>0.405786</td>\n",
       "      <td>0.401221</td>\n",
       "      <td>0.407660</td>\n",
       "      <td>0.437916</td>\n",
       "      <td>0.401258</td>\n",
       "      <td>0.435040</td>\n",
       "      <td>0.401320</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows Ã— 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             learning      deep  networks    neural       and        as  \\\n",
       "learning     0.000000  0.564340  0.520465  0.687309  0.702848  0.677681   \n",
       "deep         0.564340  0.000000  0.743636  0.701144  0.670640  0.683945   \n",
       "networks     0.520465  0.743636  0.000000  0.678898  0.697930  0.663260   \n",
       "neural       0.687309  0.701144  0.678898  0.000000  0.472190  0.356580   \n",
       "and          0.702848  0.670640  0.697930  0.472190  0.000000  0.430277   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "surpassing   0.728790  0.704478  0.738150  0.425865  0.431645  0.435908   \n",
       "human        0.722269  0.693541  0.694240  0.427338  0.416170  0.400384   \n",
       "expert       0.731950  0.752698  0.688614  0.395312  0.421400  0.402563   \n",
       "performance  0.675630  0.738534  0.644455  0.447651  0.412978  0.394644   \n",
       "PAD          0.765723  0.722181  0.709829  0.457611  0.376110  0.443575   \n",
       "\n",
       "                   of   machine  supervised      have  ...   results  \\\n",
       "learning     0.736050  0.714989    0.718350  0.959826  ...  0.707415   \n",
       "deep         0.715158  0.725764    0.659227  0.914696  ...  0.702825   \n",
       "networks     0.728734  0.693935    0.675196  0.914873  ...  0.707060   \n",
       "neural       0.418329  0.407396    0.380304  0.650154  ...  0.407154   \n",
       "and          0.406545  0.419735    0.382753  0.669322  ...  0.401106   \n",
       "...               ...       ...         ...       ...  ...       ...   \n",
       "surpassing   0.397487  0.413895    0.392436  0.689252  ...  0.395883   \n",
       "human        0.435623  0.393451    0.412255  0.715151  ...  0.431959   \n",
       "expert       0.424996  0.411999    0.408820  0.722799  ...  0.387381   \n",
       "performance  0.406125  0.395811    0.401579  0.666266  ...  0.362889   \n",
       "PAD          0.388459  0.430730    0.370831  0.687655  ...  0.408633   \n",
       "\n",
       "             comparable        in      some     cases  surpassing     human  \\\n",
       "learning       0.642867  0.712345  0.662297  0.745983    0.728790  0.722269   \n",
       "deep           0.670269  0.678129  0.623978  0.744906    0.704478  0.693541   \n",
       "networks       0.634636  0.642973  0.671950  0.676299    0.738150  0.694240   \n",
       "neural         0.363231  0.405016  0.365781  0.442707    0.425865  0.427338   \n",
       "and            0.399500  0.443710  0.380114  0.420614    0.431645  0.416170   \n",
       "...                 ...       ...       ...       ...         ...       ...   \n",
       "surpassing     0.441815  0.399707  0.380512  0.441793    0.000000  0.440634   \n",
       "human          0.377919  0.379137  0.363673  0.407000    0.440634  0.000000   \n",
       "expert         0.399963  0.357308  0.391456  0.418710    0.398612  0.422697   \n",
       "performance    0.418886  0.409257  0.425441  0.381308    0.424760  0.441135   \n",
       "PAD            0.424597  0.405786  0.401221  0.407660    0.437916  0.401258   \n",
       "\n",
       "               expert  performance       PAD  \n",
       "learning     0.731950     0.675630  0.765723  \n",
       "deep         0.752698     0.738534  0.722181  \n",
       "networks     0.688614     0.644455  0.709829  \n",
       "neural       0.395312     0.447651  0.457611  \n",
       "and          0.421400     0.412978  0.376110  \n",
       "...               ...          ...       ...  \n",
       "surpassing   0.398612     0.424760  0.437916  \n",
       "human        0.422697     0.441135  0.401258  \n",
       "expert       0.000000     0.413012  0.435040  \n",
       "performance  0.413012     0.000000  0.401320  \n",
       "PAD          0.435040     0.401320  0.000000  \n",
       "\n",
       "[75 rows x 75 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "data = pd.DataFrame(distance_matrix,index=word2id.keys())\n",
    "data.columns = word2id.keys()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b9b0aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchWord(WordList):\n",
    "    similar_words ={}\n",
    "    for search_term in WordList:\n",
    "        if(search_term in word2id.keys()):\n",
    "            similar_words[search_term]=[id2word[idx] for idx in \n",
    "                                        distance_matrix[word2id[search_term]-1].argsort()[0:5]+1] \n",
    "    return similar_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec9bbe76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deep': ['deep', 'learning', 'processing', 'some', 'supervised']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SearchWord(['deep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da35596c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
